{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "\n",
    "# from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES # for spaCy 2.1 and earlier \n",
    "from spacy.lang.en import English   # for spaCy 2.2\n",
    "\n",
    "# lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES) # for spaCy 2.1 and earlier \n",
    "lemmatizer = English.Defaults.create_lemmatizer()   # for spaCy 2.2\n",
    "\n",
    "import neuralcoref\n",
    "nlp.add_pipe(neuralcoref.NeuralCoref(nlp.vocab,blacklist=False),name=\"neuralcoref\")\n",
    "\n",
    "from main2 import ConnoFramer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add File Path to Desired Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be replaced with file path to custom lexicon \n",
    "lexicon_path = './FramesAgencyPower/agency_power.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Small demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_stories = [\"I was just thinking about walking down the street, when my shoelace snapped. I had to call my doctor to pick me up. I felt so bad I also called my friend Katie, who came in her car. She was a lifesaver. My friend Jack is nice.\",\n",
    "                   \"My doctor fixed my shoe. I thanked him. Then Susan arrived. Now she is calling the doctor too.\"]\n",
    "text_ids = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 27.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-20 20:32:38 Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "framer = ConnoFramer()\n",
    "framer.load_lexicon(lexicon_path, 'verb', 'power')\n",
    "framer.train(example_stories,\n",
    "             text_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Scores For All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function main2.ConnoFramer.__score_dataset.<locals>.<lambda>()>,\n",
       "            {'i': defaultdict(int, {'positive': 0, 'negative': 2}),\n",
       "             'my doctor': defaultdict(int, {'positive': 4, 'negative': 0}),\n",
       "             'my': defaultdict(int, {'positive': 0, 'negative': 1}),\n",
       "             'susan': defaultdict(int, {'positive': 0, 'negative': 1})})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framer.get_score_totals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was just thinking about walking down the street, when my shoelace snapped. I had to call my doctor to pick me up. I felt so bad I also called my friend Katie, who came in her car. She was a lifesaver. My friend Jack is nice.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_stories[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Specific Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Scores for Specific Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function main2.ConnoFramer.__score_document.<locals>.<lambda>()>,\n",
       "            {'i': defaultdict(int, {'negative': 2, 'positive': 0}),\n",
       "             'my doctor': defaultdict(int, {'positive': 1, 'negative': 0})})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framer.get_scores_for_doc(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function main2.ConnoFramer.__score_document.<locals>.<lambda>()>,\n",
       "            {'my': defaultdict(int, {'negative': 1, 'positive': 0}),\n",
       "             'my doctor': defaultdict(int, {'positive': 3, 'negative': 0}),\n",
       "             'susan': defaultdict(int, {'negative': 1, 'positive': 0})})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framer.get_scores_for_doc(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Count Noun Subjects for Specific Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('my doctor', 'be'): 1,\n",
       "             ('i', 'think'): 1,\n",
       "             ('i', 'have'): 1,\n",
       "             ('i', 'feel'): 1,\n",
       "             ('i', 'call'): 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framer.count_nsubj_for_doc(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('my', 'thank'): 1,\n",
       "             ('my doctor', 'fix'): 1,\n",
       "             ('susan', 'arrive'): 1,\n",
       "             ('susan', 'call'): 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framer.count_nsubj_for_doc(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Count Direct Objects for Specific Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {('my doctor', 'call'): 1, ('i', 'pick'): 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framer.count_dobj_for_doc(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My doctor fixed my shoe. I thanked him. Then Susan arrived. Now she is calling the doctor too.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_stories[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {('my doctor', 'thank'): 1, ('my doctor', 'call'): 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framer.count_dobj_for_doc(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Bigger demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "text_ids = []\n",
    "stories_path = '/Users/maria/Documents/data/narrativity/litbank/original'   # Litbank corpus here: https://github.com/dbamman/litbank\n",
    "\n",
    "j = 0\n",
    "for _file_name in os.listdir(stories_path):\n",
    "    _lines = []\n",
    "    for _line in open(stories_path + '/' + _file_name, 'r'):\n",
    "        if _line.strip():\n",
    "            _lines.append(_line.strip())\n",
    "\n",
    "    # Randomly sample 100 paragraphs from each book\n",
    "    for _line in random.sample(_lines, 100):        \n",
    "        texts.append(_line)\n",
    "        text_ids.append(j)\n",
    "        j += 1\n",
    "\n",
    "len(texts), len(text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-06 12:07:08 Processed 0 out of 10000\n",
      "2023-02-06 12:07:09 Processed 100 out of 10000\n",
      "2023-02-06 12:07:10 Processed 200 out of 10000\n",
      "2023-02-06 12:07:11 Processed 300 out of 10000\n",
      "2023-02-06 12:07:12 Processed 400 out of 10000\n",
      "2023-02-06 12:07:13 Processed 500 out of 10000\n",
      "2023-02-06 12:07:14 Processed 600 out of 10000\n",
      "2023-02-06 12:07:15 Processed 700 out of 10000\n",
      "2023-02-06 12:07:16 Processed 800 out of 10000\n",
      "2023-02-06 12:07:16 Processed 900 out of 10000\n",
      "2023-02-06 12:07:17 Processed 1000 out of 10000\n",
      "2023-02-06 12:07:18 Processed 1100 out of 10000\n",
      "2023-02-06 12:07:19 Processed 1200 out of 10000\n",
      "2023-02-06 12:07:20 Processed 1300 out of 10000\n",
      "2023-02-06 12:07:21 Processed 1400 out of 10000\n",
      "2023-02-06 12:07:22 Processed 1500 out of 10000\n",
      "2023-02-06 12:07:23 Processed 1600 out of 10000\n",
      "2023-02-06 12:07:24 Processed 1700 out of 10000\n",
      "2023-02-06 12:07:24 Processed 1800 out of 10000\n",
      "2023-02-06 12:07:25 Processed 1900 out of 10000\n",
      "2023-02-06 12:07:26 Processed 2000 out of 10000\n",
      "2023-02-06 12:07:27 Processed 2100 out of 10000\n",
      "2023-02-06 12:07:28 Processed 2200 out of 10000\n",
      "2023-02-06 12:07:29 Processed 2300 out of 10000\n",
      "2023-02-06 12:07:30 Processed 2400 out of 10000\n",
      "2023-02-06 12:07:31 Processed 2500 out of 10000\n",
      "2023-02-06 12:07:31 Processed 2600 out of 10000\n",
      "2023-02-06 12:07:32 Processed 2700 out of 10000\n",
      "2023-02-06 12:07:33 Processed 2800 out of 10000\n",
      "2023-02-06 12:07:34 Processed 2900 out of 10000\n",
      "2023-02-06 12:07:35 Processed 3000 out of 10000\n",
      "2023-02-06 12:07:36 Processed 3100 out of 10000\n",
      "2023-02-06 12:07:37 Processed 3200 out of 10000\n",
      "2023-02-06 12:07:38 Processed 3300 out of 10000\n",
      "2023-02-06 12:07:39 Processed 3400 out of 10000\n",
      "2023-02-06 12:07:40 Processed 3500 out of 10000\n",
      "2023-02-06 12:07:41 Processed 3600 out of 10000\n",
      "2023-02-06 12:07:41 Processed 3700 out of 10000\n",
      "2023-02-06 12:07:42 Processed 3800 out of 10000\n",
      "2023-02-06 12:07:43 Processed 3900 out of 10000\n",
      "2023-02-06 12:07:44 Processed 4000 out of 10000\n",
      "2023-02-06 12:07:45 Processed 4100 out of 10000\n",
      "2023-02-06 12:07:46 Processed 4200 out of 10000\n",
      "2023-02-06 12:07:47 Processed 4300 out of 10000\n",
      "2023-02-06 12:07:48 Processed 4400 out of 10000\n",
      "2023-02-06 12:07:49 Processed 4500 out of 10000\n",
      "2023-02-06 12:07:50 Processed 4600 out of 10000\n",
      "2023-02-06 12:07:51 Processed 4700 out of 10000\n",
      "2023-02-06 12:07:52 Processed 4800 out of 10000\n",
      "2023-02-06 12:07:52 Processed 4900 out of 10000\n",
      "2023-02-06 12:07:53 Processed 5000 out of 10000\n",
      "2023-02-06 12:07:54 Processed 5100 out of 10000\n",
      "2023-02-06 12:07:55 Processed 5200 out of 10000\n",
      "2023-02-06 12:07:56 Processed 5300 out of 10000\n",
      "2023-02-06 12:07:57 Processed 5400 out of 10000\n",
      "2023-02-06 12:07:58 Processed 5500 out of 10000\n",
      "2023-02-06 12:07:59 Processed 5600 out of 10000\n",
      "2023-02-06 12:08:00 Processed 5700 out of 10000\n",
      "2023-02-06 12:08:01 Processed 5800 out of 10000\n",
      "2023-02-06 12:08:02 Processed 5900 out of 10000\n",
      "2023-02-06 12:08:03 Processed 6000 out of 10000\n",
      "2023-02-06 12:08:04 Processed 6100 out of 10000\n",
      "2023-02-06 12:08:05 Processed 6200 out of 10000\n",
      "2023-02-06 12:08:06 Processed 6300 out of 10000\n",
      "2023-02-06 12:08:06 Processed 6400 out of 10000\n",
      "2023-02-06 12:08:07 Processed 6500 out of 10000\n",
      "2023-02-06 12:08:08 Processed 6600 out of 10000\n",
      "2023-02-06 12:08:09 Processed 6700 out of 10000\n",
      "2023-02-06 12:08:10 Processed 6800 out of 10000\n",
      "2023-02-06 12:08:11 Processed 6900 out of 10000\n",
      "2023-02-06 12:08:12 Processed 7000 out of 10000\n",
      "2023-02-06 12:08:13 Processed 7100 out of 10000\n",
      "2023-02-06 12:08:14 Processed 7200 out of 10000\n",
      "2023-02-06 12:08:14 Processed 7300 out of 10000\n",
      "2023-02-06 12:08:15 Processed 7400 out of 10000\n",
      "2023-02-06 12:08:16 Processed 7500 out of 10000\n",
      "2023-02-06 12:08:17 Processed 7600 out of 10000\n",
      "2023-02-06 12:08:18 Processed 7700 out of 10000\n",
      "2023-02-06 12:08:19 Processed 7800 out of 10000\n",
      "2023-02-06 12:08:20 Processed 7900 out of 10000\n",
      "2023-02-06 12:08:21 Processed 8000 out of 10000\n",
      "2023-02-06 12:08:21 Processed 8100 out of 10000\n",
      "2023-02-06 12:08:22 Processed 8200 out of 10000\n",
      "2023-02-06 12:08:23 Processed 8300 out of 10000\n",
      "2023-02-06 12:08:24 Processed 8400 out of 10000\n",
      "2023-02-06 12:08:25 Processed 8500 out of 10000\n",
      "2023-02-06 12:08:26 Processed 8600 out of 10000\n",
      "2023-02-06 12:08:27 Processed 8700 out of 10000\n",
      "2023-02-06 12:08:28 Processed 8800 out of 10000\n",
      "2023-02-06 12:08:28 Processed 8900 out of 10000\n",
      "2023-02-06 12:08:29 Processed 9000 out of 10000\n",
      "2023-02-06 12:08:30 Processed 9100 out of 10000\n",
      "2023-02-06 12:08:31 Processed 9200 out of 10000\n",
      "2023-02-06 12:08:32 Processed 9300 out of 10000\n",
      "2023-02-06 12:08:33 Processed 9400 out of 10000\n",
      "2023-02-06 12:08:34 Processed 9500 out of 10000\n",
      "2023-02-06 12:08:35 Processed 9600 out of 10000\n",
      "2023-02-06 12:08:35 Processed 9700 out of 10000\n",
      "2023-02-06 12:08:36 Processed 9800 out of 10000\n",
      "2023-02-06 12:08:37 Processed 9900 out of 10000\n",
      "2023-02-06 12:08:38 Complete!\n"
     ]
    }
   ],
   "source": [
    "framer = ConnoFramer()\n",
    "framer.load_lexicon(lexicon_path, 'verb', 'power')\n",
    "framer.train(texts,\n",
    "             text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persona_score_dict = framer.get_score_totals()\n",
    "len(persona_score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persona_sum_dict = {_persona: _category_score_dict['positive']-_category_score_dict['negative'] for _persona, _category_score_dict in persona_score_dict.items()}\n",
    "len(persona_sum_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "138 260 122\n",
      "\n",
      "you\n",
      "20 51 31\n",
      "\n",
      "charles\n",
      "4 4 0\n",
      "\n",
      "mrs. todd\n",
      "4 4 0\n",
      "\n",
      "margaret\n",
      "3 4 1\n",
      "\n",
      "carol\n",
      "3 3 0\n",
      "\n",
      "anne\n",
      "3 3 0\n",
      "\n",
      "tarzan\n",
      "3 3 0\n",
      "\n",
      "mitchell\n",
      "2 2 0\n",
      "\n",
      "noah\n",
      "2 2 0\n",
      "\n",
      "frank\n",
      "2 2 0\n",
      "\n",
      "mr. casaubon\n",
      "2 2 0\n",
      "\n",
      "mr. hale\n",
      "2 2 0\n",
      "\n",
      "mrs. radford\n",
      "2 2 0\n",
      "\n",
      "mrs. dean\n",
      "2 2 0\n",
      "\n",
      "laurie\n",
      "2 2 0\n",
      "\n",
      "meg\n",
      "2 2 0\n",
      "\n",
      "a doctor\n",
      "2 2 0\n",
      "\n",
      "john\n",
      "2 2 0\n",
      "\n",
      "jock\n",
      "2 3 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _persona, _sum in sorted(persona_sum_dict.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(_persona)\n",
    "    print(_sum, persona_score_dict[_persona]['positive'], persona_score_dict[_persona]['negative'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "66412c03f91e9a42e9c41dd543b50b96eee08a8e3011708689019231b0abadf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
